from sklearn.inspection import permutation_importance
from matplotlib import pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
import os
import numpy as np
from tqdm import tqdm
from glob import glob
from random import sample

import librosa
import soundfile as sf

import torch
import opensmile

from sklearn.svm import SVC
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import classification_report

import warnings
warnings.filterwarnings('ignore')


# Defining a function for loading and resampling audio files

def load_audio_files(audio_files, resampling_frequency=16000, audio_list=None):
  '''
  Loads and resamples audio files 
  
  Parameters
  ------------
  audio_files: string
      The paths of the wav files 
  resampling_frequency: integer
      The frequency which all audios will be resampled to
  audio_list: list 
      The list of torch tensors of audios to which more audios need too be added, empty by default

  Returns
  ------------
  audio_list: list
      A list of torch tensors, one array for each audio file

  '''
  # Making audio_list
  if audio_list is None:
    audio_list = []

  # Resampling
  for audio in audio_files:
    signal, fs = librosa.load(audio, sr=resampling_frequency)
    audio_list.append(torch.from_numpy(signal))

  return audio_list


def get_egemaps_feature_names():
  model = opensmile.Smile(
      feature_set=opensmile.FeatureSet.eGeMAPSv02,
      feature_level=opensmile.FeatureLevel.Functionals,
  )
  egemaps_feature_names = model.column_names
  return egemaps_feature_names


# Defining a function for generating audio embedding extraction models

def audio_embeddings_model(model_name):
  '''
  Generates model for embedding extraction 
  
  Parameters
  ------------
  mode_name: string
      The model to used, could be 'hybrid_byols', 'compare' or 'egemaps'

  Returns
  ------------
  model: object
      The embedding extraction model
  '''
  if model_name == 'hybrid_byols':
    model_name = 'cvt'
    checkpoint_path = "serab-byols/checkpoints/cvt_s1-d1-e64_s2-d1-e256_s3-d1-e512_BYOLAs64x96-osandbyolaloss6373-e100-bs256-lr0003-rs42.pth"
    model = serab_byols.load_model(checkpoint_path, model_name)
  elif model_name == 'compare':
    model = opensmile.Smile(
        feature_set=opensmile.FeatureSet.ComParE_2016,
        feature_level=opensmile.FeatureLevel.Functionals,
    )
  elif model_name == 'egemaps':
    model = opensmile.Smile(
        feature_set=opensmile.FeatureSet.eGeMAPSv02,
        feature_level=opensmile.FeatureLevel.Functionals,
    )
  return model


# Defining a function for embedding exctraction from the audio list

def audio_embeddings(audio_list, model_name, model, sampling_rate=16000):
  '''
  Loads and resamples audio files 
  
  Parameters
  ------------
  audio_list: list
      A list of arrays, one array for each audio file
  model_name: string
      The model to used, could be 'hybrid_byols', 'compare' or 'egemaps'
  model: object
      The embedding extraction model generated by audio_embeddings_model function
  sampling_rate: int
      The sampling rate, 16 kHz by default

  Returns
  ------------
  embeddings_array: array
      The array containg embeddings of all audio_files, dimension (number of audio files × n_feats)
      
  '''
  if model_name == 'hybrid_byols':
    embeddings_array = serab_byols.get_scene_embeddings(audio_list, model)
  else:
    embeddings_list = []
    for i in tqdm(range(len(audio_list))):
      embeddings = model.process_signal(audio_list[i], sampling_rate)
      embeddings_list.append(torch.tensor(
          embeddings.values[0], dtype=torch.float32))
    embeddings_array = torch.stack(embeddings_list)
  return embeddings_array

# Defining a function for speaker normalisation using standard scaler


def speaker_normalisation(embeddings_array, speakers):
  '''
  Normalises embeddings_array for each speaker
  
  Parameters
  ------------
  embeddings_array: array
      The array of embeddings, one row for each audio file
  speakers: list 
      The list of speakers

  Returns
  ------------
  embeddings_array: array
      The array containg normalised embeddings of all audio_files, dimension (number of audio files × n_feats)
      
  '''
  speaker_ids = set(speakers)
  for speaker_id in speaker_ids:
    speaker_embeddings_indices = np.where(np.array(speakers) == speaker_id)[0]
    speaker_embeddings = embeddings_array[speaker_embeddings_indices, :]
    scaler = StandardScaler()
    normalised_speaker_embeddings = scaler.fit_transform(speaker_embeddings)
    embeddings_array[speaker_embeddings_indices] = torch.tensor(
        normalised_speaker_embeddings).float()
  return embeddings_array

  # Defining a function for selecting a part of the dataset containing only the required labels


# Defining a function for selecting a part of the dataset containing only the required labels

def label_division(embeddings_array, labels, required_labels, speakers):
  '''
  Normalises embeddings_array for each speaker
  
  Parameters
  ------------
  embeddings_array: array
      The array of embeddings, one row for each audio file
  speakers: list 
      The list of speakers

  Returns
  ------------
  embeddings_array: array
      The array containg normalised embeddings of all audio_files, dimension (number of audio files × n_feats)
      
  '''
  final_embeddings_array = {}
  final_labels_list = {}
  final_speakers_list = []
  for label in required_labels:
    # print(label)
    label_indices = np.where(np.array(labels) == label)[0]
    # print(label_indices)
    label_embeddings = embeddings_array[label_indices, :]
    # print(label_embeddings.shape)
    final_speakers_list.extend(list(np.array(speakers)[label_indices]))
    final_embeddings_array[label] = label_embeddings
    final_labels_list[label] = [label]*len(label_indices)
    # print(final_labels_list[label])
  final_embeddings = np.vstack(
      (final_embeddings_array[required_labels[0]], final_embeddings_array[required_labels[1]]))
  final_labels = final_labels_list[required_labels[0]
                                   ]+final_labels_list[required_labels[1]]
  # print(final_embeddings.shape)
  # print(len(final_labels))
  # print(len(final_speakers_list))
  # print(final_speakers_list)
  return final_embeddings, final_labels, final_speakers_list


# Defining a function for splitting into train set and test set with diferent speakers in each set
def split_train_test(normalised_embeddings_array, labels, speakers, test_size=0.30):
  '''
  Splits into training and testing set with different speakers

  Parameters
  ------------
  normalised_embeddings_array: torch tensor
    The tensor containing normalised embeddings 
  labels: list of strings
    The list of emotions corresponding to audio files
  speakers: list 
    The list of speakers
  test_size: float 
    The fraction of embeddings and labels to put in the test set

  Returns
  ------------
  X_train: torch tensor
    The normalised embeddings that will be used for training
  X_test: torch tensor
    The normalised embeddings that will be used for testing
  y_train: list
   The labels that will be used for training
  y_test: list
   The labels that will be used for testing

  '''
  # unique speakers in this dataset
  all_speakers = set(speakers)
  # unique speakers in test set
  test_speakers = sample(all_speakers, int(test_size*len(all_speakers)))

  test_speakers_indices = []
  train_speakers_indices = []

  for speaker in all_speakers:
      if speaker in test_speakers:
          speaker_indices = np.where(np.array(speakers) == speaker)[0]
          test_speakers_indices.extend(speaker_indices)
      else:
          speaker_indices = np.where(np.array(speakers) == speaker)[0]
          train_speakers_indices.extend(speaker_indices)

  X_train = normalised_embeddings_array[train_speakers_indices]
  X_test = normalised_embeddings_array[test_speakers_indices]

  y_train = [0 for i in range(len(train_speakers_indices))]
  y_test = [0 for i in range(len(test_speakers_indices))]

  for i, index in enumerate(train_speakers_indices):
      y_train[i] = labels[index]
  for i, index in enumerate(test_speakers_indices):
      y_test[i] = labels[index]

  return X_train, X_test, y_train, y_test


# Defining a function for hyperparameter tuning and getting the accuracy on the test set
def get_hyperparams(X_train, X_test, y_train, y_test, classifier, parameters):
  '''
  Splits into training and testing set with different speakers

  Parameters
  ------------
  X_train: torch tensor
    The normalised embeddings that will be used for training
  X_test: torch tensor
    The normalised embeddings that will be used for testing
  y_train: list
    The labels that will be used for training
  y_test: list
    The labels that will be used for testing
  classifier: object
    The instance of the classification model 
  parameters: dictionary
    The dictionary of parameters for GridSearchCV 

  Returns
  ------------
    The dictionary of the best hyperparameters
  
  '''
  grid = GridSearchCV(classifier, param_grid=parameters,
                      cv=5, scoring='recall_macro')
  grid.fit(X_train, y_train)
  # print('recall_macro :',grid.best_score_)
  # print('Best Parameters: {}'.format(grid.best_params_))
  print('recall_macro on test_set: {}'.format(grid.score(X_test, y_test)))
  predictions = grid.predict(X_test)
  # print(classification_report(y_test, predictions))
  return grid.score(X_test, y_test)


# Defining a function for all steps
def pipeline(labeled_array, speakers, labels_list, classification_model):
  '''
  Loads and resamples audio files 
  
  Parameters
  ------------
  audio_files: string
      The paths of the wav files 
  resampling_frequency: integer
      The frequency which all audios will be resampled to
  audio_list: list 
      The list of torch tensors of audios to which more audios need too be added, empty by default

  Returns
  ------------
  audio_list: list
      A list of torch tensors, one array for each audio file

  '''

  # print('MODEL: {}'.format(model_name))

  # Train Test Splitting
  X_train, X_test, y_train, y_test = split_train_test(
      labeled_array, labels_list, speakers, test_size=0.30)
  # print('X_train shape: {}'.format(X_train.shape))
  # print('X_test shape: {}'.format(X_test.shape))
  # print('y_train len: {}'.format(len(y_train)))
  # print('y_test len: {}'.format(len(y_test)))
  # print()

  # Getting hyperparameters and checking max_recall
  if classification_model == 'logreg':
    classifier = LogisticRegression()
    parameters = {'penalty': [
        'l1', 'l2'], 'C': np.logspace(-3, 1, 3), 'solver': ['lbfgs', 'sag']}
    result = get_hyperparams(
        X_train, X_test, y_train, y_test, classifier, parameters)
  elif classification_model == 'svm':
    classifier = SVC()
    parameters = {
        'C': np.logspace(-2, 4, 4), 'gamma': np.logspace(-5, -3, 5), 'kernel': ['rbf']}
    result = get_hyperparams(
        X_train, X_test, y_train, y_test, classifier, parameters)
  elif classification_model == 'rf':
    classifier = RandomForestClassifier()
    parameters = {'n_estimators': [50, 100, 200], 'max_features': [
        'auto', 'log2', 'sqrt'], 'bootstrap': [False]}
    result = get_hyperparams(
        X_train, X_test, y_train, y_test, classifier, parameters)
  return result


#-------------------------------------------------------------


def permutation_importance_method(X, y, feature_names, color, top_ten=False, plot=True):
  model = KNeighborsClassifier()
  avg_importance = np.zeros((10, 88))

  model.fit(X, y)
  results = permutation_importance(model, X, y, scoring='accuracy', n_repeats=10)
  importance = results.importances_mean

  # print(importance.shape)
  actual_importance = importance
  sorted_importance = sorted(importance)
  sorted_feature_names = []
  for imp_val in sorted_importance:
    for i in range(len(actual_importance)):
      if actual_importance[i] == imp_val:
        sorted_feature_names.append(feature_names[i])
        break
  # print(sorted_feature_names)
  # print(len(sorted_feature_names))
  # print(egemaps_feature_names)
  # print(len(egemaps_feature_names))

  if plot==True:
    if (top_ten==True):
      fig, ax = plt.subplots(figsize=(12, 4))
      bars = ax.barh(sorted_feature_names[-10:], sorted_importance[-10:], color=color)
      print(sorted_feature_names[-10:])
    else:
      fig, ax = plt.subplots(figsize=(12, 20))
      bars = ax.barh(sorted_feature_names, sorted_importance, color=color)

  return importance


def NormalizeData(data):
    return (data - np.min(data)) / (np.max(data) - np.min(data))


def coef_logreg_plot(labeled_array, label_list, feature_names, color, top_ten=False, plot=True):

  classifier = LogisticRegression()

  avg_importance = np.zeros((10, 88))
  for i in range(10):
    classifier.fit(labeled_array, label_list)
    importance = classifier.coef_
    avg_importance[i] = importance[0]
  importance = np.absolute(np.mean(avg_importance, axis=0))
  importance = NormalizeData(importance)

  actual_importance = importance
  sorted_importance = sorted(importance)
  sorted_feature_names = []
  for imp_val in sorted_importance:
    for i in range(len(actual_importance)):
      if actual_importance[i] == imp_val:
        sorted_feature_names.append(feature_names[i])
        break
  # print(sorted_feature_names)
  # print(len(sorted_feature_names))
  # print(egemaps_feature_names)
  # print(len(egemaps_feature_names))
  if plot==True:
    if (top_ten==True):
      fig, ax = plt.subplots(figsize=(12, 4))
      bars = ax.barh(sorted_feature_names[-10:], sorted_importance[-10:], color=color)
      print(sorted_feature_names[-10:])
    else:
      fig, ax = plt.subplots(figsize=(12, 20))
      bars = ax.barh(sorted_feature_names, sorted_importance, color=color)

  return importance



def coef_svm_plot(labeled_array, label_list, feature_names, color, top_ten=False, plot=True):

  classifier = SVC(kernel='linear')

  avg_importance = np.zeros((10, 88))
  for i in range(10):
    classifier.fit(labeled_array, label_list)
    importance = classifier.coef_
    avg_importance[i] = importance[0]
  importance = np.absolute(np.mean(avg_importance, axis=0))
  importance = NormalizeData(importance)

  actual_importance = importance
  sorted_importance = sorted(importance)
  sorted_feature_names = []
  for imp_val in sorted_importance:
    for i in range(len(actual_importance)):
      if actual_importance[i] == imp_val:
        sorted_feature_names.append(feature_names[i])
        break
  # print(sorted_feature_names)
  # print(len(sorted_feature_names))
  # print(egemaps_feature_names)
  # print(len(egemaps_feature_names))
  if plot==True:
    if (top_ten==True):
      fig, ax = plt.subplots(figsize=(12, 4))
      bars = ax.barh(sorted_feature_names[-10:], sorted_importance[-10:], color=color)
      print(sorted_feature_names[-10:])
    else:
      fig, ax = plt.subplots(figsize=(12, 20))
      bars = ax.barh(sorted_feature_names, sorted_importance, color=color)

  return importance



def random_forest_plot(labeled_array, label_list, feature_names, color, top_ten=False, plot=True):

  classifier = RandomForestClassifier()

  avg_importance = np.zeros((10, 88))
  for i in range(10):
    classifier.fit(labeled_array, label_list)
    importance = classifier.feature_importances_
    avg_importance[i] = importance
  importance = np.mean(avg_importance, axis=0)

  actual_importance = importance
  sorted_importance = sorted(importance)
  sorted_feature_names = []
  for imp_val in sorted_importance:
    for i in range(len(actual_importance)):
      if actual_importance[i] == imp_val:
        sorted_feature_names.append(feature_names[i])
        break
  # print(sorted_feature_names)
  # print(len(sorted_feature_names))
  # print(egemaps_feature_names)
  # print(len(egemaps_feature_names))
  if plot==True:
    if (top_ten==True):
      fig, ax = plt.subplots(figsize=(12, 4))
      bars = ax.barh(sorted_feature_names[-10:], sorted_importance[-10:], color=color)
      print(sorted_feature_names[-10:])
    else:
      fig, ax = plt.subplots(figsize=(12, 20))
      bars = ax.barh(sorted_feature_names, sorted_importance, color=color)

  return importance


def combined_feature_rank(labeled_array, label_list, labeled_speakers_list, feature_names, color, emotion_list = [], top_ten=False):

  logreg_imp = coef_logreg_plot(labeled_array, label_list, feature_names=feature_names, color=color, top_ten=top_ten, plot=False)
  logreg_recall = pipeline(labeled_array, labeled_speakers_list, label_list, classification_model='logreg')

  svm_imp = coef_svm_plot(labeled_array, label_list, feature_names=feature_names, color=color, top_ten=top_ten, plot=False)
  svm_recall = pipeline(labeled_array, labeled_speakers_list, label_list, classification_model='svm')

  rf_imp = random_forest_plot(labeled_array, label_list, feature_names=feature_names, color=color, top_ten=top_ten, plot=False)
  rf_recall = pipeline(labeled_array, labeled_speakers_list, label_list, classification_model='rf')

  combined_importance = (logreg_recall*logreg_imp + svm_recall*svm_imp + rf_recall*rf_imp)/(logreg_recall+svm_recall+rf_recall)

  actual_importance = combined_importance
  sorted_importance = sorted(combined_importance)
  sorted_feature_names = []
  for imp_val in sorted_importance:
    for i in range(len(actual_importance)):
      if actual_importance[i] == imp_val:
        sorted_feature_names.append(feature_names[i])
        break

  if (top_ten==True):
    fig, ax = plt.subplots(figsize=(12, 4))
    bars = ax.barh(sorted_feature_names[-10:], sorted_importance[-10:], color=color)
    print(sorted_feature_names[-10:])
  else:
    fig, ax = plt.subplots(figsize=(12, 20))
    bars = ax.barh(sorted_feature_names, sorted_importance, color=color)

  return combined_importance
